{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df7a7918-8ff0-4f11-b929-765cb5ee6099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from skimage import transform\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from skimage import color\n",
    "\n",
    "from Multiview_autoencoder.Multiview_autoencoder import MultiviewImageDataset, MultiViewConvAutoencoder, extract_features\n",
    "from CV_CNN_classifier.CV_CNN_classifier import FeatsDataset, CNNClassifier, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a381c81e-5176-4a8b-8863-9c410be78f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Load device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13b6c876-4ff6-4097-9232-2b51ba5fdee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CV images of reaction mixture, substrate and catalyst\n",
    "image_tensors = []\n",
    "image_arrays = []\n",
    "rgb_imgs = []\n",
    "gray_imgs = []\n",
    "number = []\n",
    "image_folder = './MultiviewImageDataset/cv-mixture' \n",
    "for filename in os.listdir(image_folder):\n",
    "    if filename.endswith('.png'):\n",
    "        num = filename.split('.')[0]\n",
    "        num = int(num) - 1\n",
    "        number.append(num)\n",
    "        img = Image.open(os.path.join(image_folder, filename))\n",
    "        img_array = np.array(img)\n",
    "        if len(img_array.shape) == 3:\n",
    "            #print(num)\n",
    "            rgb_image = color.rgba2rgb(img)\n",
    "            rgb_imgs.append(rgb_image)\n",
    "            gray_img = color.rgb2gray(rgb_image)\n",
    "            gray_imgs.append(gray_img)\n",
    "            img_array = np.array(gray_img)\n",
    "        image_arrays.append(img_array)\n",
    "        new_size = (400, 400)\n",
    "        resized_img = transform.resize(img_array, new_size)\n",
    "        img_tensor = torch.tensor(resized_img)\n",
    "        img_tensor = img_tensor.to(torch.float32)\n",
    "        image_tensors.append(img_tensor)\n",
    "\n",
    "image_tensors_sub = []\n",
    "image_arrays_sub = []\n",
    "rgb_imgs_sub = []\n",
    "gray_imgs_sub = []\n",
    "number_sub = []\n",
    "sub_image_folder = './MultiviewImageDataset/cv-sub' \n",
    "for filename in os.listdir(sub_image_folder):\n",
    "    if filename.endswith('.png'):\n",
    "        num = filename.split('-')[0]\n",
    "        num = int(num) - 1\n",
    "        number_sub.append(num)\n",
    "        img = Image.open(os.path.join(sub_image_folder, filename))\n",
    "        img_array = np.array(img)\n",
    "        if len(img_array.shape) == 3:\n",
    "            #print(num)\n",
    "            rgb_image = color.rgba2rgb(img)\n",
    "            rgb_imgs_sub.append(rgb_image)\n",
    "            gray_img = color.rgb2gray(rgb_image)\n",
    "            gray_imgs_sub.append(gray_img)\n",
    "            img_array = np.array(gray_img)\n",
    "        image_arrays_sub.append(img_array)\n",
    "        new_size = (400, 400)\n",
    "        resized_img = transform.resize(img_array, new_size)\n",
    "        img_tensor = torch.tensor(resized_img)\n",
    "        img_tensor = img_tensor.to(torch.float32)\n",
    "        image_tensors_sub.append(img_tensor)\n",
    "\n",
    "sub_dict = {}\n",
    "for idx, sub in zip(number_sub,image_tensors_sub):\n",
    "    sub_dict[idx] = sub\n",
    "sub_train = []\n",
    "for i in number:\n",
    "    y = sub_dict[i]\n",
    "    sub_train.append(y)        \n",
    "\n",
    "image_tensors_cat = []\n",
    "image_arrays_cat = []\n",
    "rgb_imgs_cat = []\n",
    "gray_imgs_cat = []\n",
    "number_cat = []\n",
    "cat_image_folder = './MultiviewImageDataset/cv-cat'\n",
    "for filename in os.listdir(cat_image_folder):\n",
    "    if filename.endswith('.png'):\n",
    "        num = filename.split('-')[0]\n",
    "        num = int(num) - 1\n",
    "        number_cat.append(num)\n",
    "        img = Image.open(os.path.join(cat_image_folder, filename))\n",
    "        img_array = np.array(img)\n",
    "        if len(img_array.shape) == 3:\n",
    "            #print(num)\n",
    "            rgb_image = color.rgba2rgb(img)\n",
    "            rgb_imgs_cat.append(rgb_image)\n",
    "            gray_img = color.rgb2gray(rgb_image)\n",
    "            gray_imgs_cat.append(gray_img)\n",
    "            img_array = np.array(gray_img)\n",
    "        image_arrays_sub.append(img_array)\n",
    "        new_size = (400, 400)\n",
    "        resized_img = transform.resize(img_array, new_size)\n",
    "        img_tensor = torch.tensor(resized_img)\n",
    "        img_tensor = img_tensor.to(torch.float32)\n",
    "        image_tensors_cat.append(img_tensor)\n",
    "\n",
    "cat_dict = {}\n",
    "for idx, cat in zip(number_cat,image_tensors_cat):\n",
    "    cat_dict[idx] = cat\n",
    "cat_train = []\n",
    "for i in number:\n",
    "    y = cat_dict[i]\n",
    "    cat_train.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2c86c30-741f-45f6-9cd6-fa3edfc4b68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load learning target\n",
    "df = pd.read_csv('./MultiviewImageDataset/data_all.csv')\n",
    "target = df['tag'].to_list()\n",
    "target = np.array(target)\n",
    "\n",
    "num_csv = df['num'].to_list()\n",
    "num_csv = np.array(num_csv)\n",
    "\n",
    "target_dict = {}\n",
    "for idx, y in zip(num_csv, target):\n",
    "    target_dict[idx] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52fdbea9-a7a3-48e8-9628-ed6596c80db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder_epoch [1], Loss: 0.0042\n",
      "Autoencoder_epoch [2], Loss: 0.0074\n",
      "Autoencoder_epoch [3], Loss: 0.0019\n",
      "Autoencoder_epoch [4], Loss: 0.0011\n",
      "Autoencoder_epoch [5], Loss: 0.0002\n",
      "Early stopping at Autoencoder_epoch 5, loss reached 0.000169\n"
     ]
    }
   ],
   "source": [
    "# Training autoencoder\n",
    "dataset = MultiviewImageDataset(image_tensors, sub_train, cat_train, number)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True)    \n",
    "model = MultiViewConvAutoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "\n",
    "num_train = []\n",
    "decoding_list1 = []\n",
    "decoding_list2 = []\n",
    "decoding_list3 = []\n",
    "for epoch in range(30):\n",
    "    stop_training = False\n",
    "    for img1, img2, img3, num in data_loader:\n",
    "        img1, img2, img3 = img1.to(device), img2.to(device), img3.to(device)\n",
    "        if epoch == 29:\n",
    "            num_train.append(num)       \n",
    "        optimizer.zero_grad()\n",
    "        rec1, rec2, rec3, _ = model(img1, img2, img3)\n",
    "        if epoch == 29:\n",
    "            decoding_list1.append(rec1)\n",
    "            decoding_list2.append(rec2)\n",
    "            decoding_list3.append(rec3)\n",
    "        img1 = img1.unsqueeze(1)\n",
    "        img2 = img2.unsqueeze(1)\n",
    "        img3 = img3.unsqueeze(1)\n",
    "        loss1 = criterion(rec1, img1)\n",
    "        loss2 = criterion(rec2, img2)\n",
    "        loss3 = criterion(rec3, img3)\n",
    "        loss_total = loss1 + loss2 + loss3 \n",
    "        loss_total.backward()\n",
    "        optimizer.step()\n",
    "        if loss_total.item() < 0.0002:\n",
    "            stop_training = True\n",
    "            break \n",
    "    print(f'Autoencoder_epoch [{epoch+1}], Loss: {loss_total.item():.4f}')\n",
    "    if stop_training:\n",
    "        print(f\"Early stopping at Autoencoder_epoch {epoch+1}, loss reached {loss_total.item():.6f}\")\n",
    "        break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24d9bf14-9894-4251-aac2-5d91aba80311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract compressed features of the autoencoder\n",
    "shared_features, num_list = extract_features(model, data_loader, device)\n",
    "shared_features = shared_features.cpu()\n",
    "\n",
    "target_AE = []\n",
    "for i in num_list:\n",
    "    i = i.numpy()\n",
    "    y = target_dict[(i[0])]\n",
    "    target_AE.append(y)    \n",
    "target_AE = np.array(target_AE)\n",
    "dataset_f = FeatsDataset(shared_features, num_list, target_AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "030f1b5c-9651-44b9-b5e7-32e181e1b657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 01, Validation Acc. 0.8000, Loss 0.0125 \n",
      "Fold 02, Validation Acc. 1.0000, Loss 0.0027 \n",
      "Fold 03, Validation Acc. 1.0000, Loss 0.0324 \n",
      "Fold 04, Validation Acc. 1.0000, Loss 0.0000 \n",
      "Fold 05, Validation Acc. 1.0000, Loss 0.0332 \n",
      "Fold 06, Validation Acc. 1.0000, Loss 0.0257 \n",
      "Fold 07, Validation Acc. 1.0000, Loss 0.0035 \n",
      "Fold 08, Validation Acc. 0.8000, Loss 0.0108 \n",
      "Fold 09, Validation Acc. 1.0000, Loss 0.0009 \n",
      "Fold 10, Validation Acc. 1.0000, Loss 0.0273 \n",
      "Accuracy_all 0.9800, Loss 0.0149\n"
     ]
    }
   ],
   "source": [
    "# Training CNN classifier\n",
    "classifier = CNNClassifier()\n",
    "classifier = classifier.to(device)\n",
    "criterion_c = nn.CrossEntropyLoss()\n",
    "optimizer_c = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "kf = KFold(n_splits=10, shuffle=True,random_state=0)\n",
    "\n",
    "all_acc_list = []\n",
    "all_loss_list = []\n",
    "acc_train_list = []\n",
    "fold_num = 1\n",
    "for train_indices, val_indices in kf.split(dataset_f):\n",
    "        \n",
    "    train_dataset = [dataset_f[i] for i in train_indices]\n",
    "    val_dataset = [dataset_f[i] for i in val_indices]\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True) \n",
    "    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n",
    "    \n",
    "    predicted_val_list = []\n",
    "    predicted_val_list = np.array(predicted_val_list)\n",
    "    labels_val_list = []\n",
    "    num_val_list = []\n",
    "    acc_val_list= []\n",
    "    loss_list = []\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        for image, num, label in train_loader:\n",
    "            image = image.to(device, dtype=torch.float32, non_blocking=True)\n",
    "            label = label.to(device, non_blocking=True)\n",
    "            optimizer_c.zero_grad()\n",
    "            output = classifier(image)\n",
    "            label = label.view(-1)\n",
    "            loss = criterion_c(output, label)\n",
    "            loss.backward()\n",
    "            optimizer_c.step()\n",
    "        label_val, prediction_val, num_val = evaluate(classifier, val_loader, device)\n",
    "        label_train, prediction_train, num_train = evaluate(classifier, train_loader, device)\n",
    "        label_train = np.array(label_train)\n",
    "        acc_train = metrics.accuracy_score(label_train, prediction_train)\n",
    "        acc_train_list.append(acc_train)\n",
    "        label_val = np.array(label_val)\n",
    "        acc_val = metrics.accuracy_score(label_val, prediction_val)\n",
    "        acc_val_list.append(acc_val)\n",
    "        loss = loss.detach()\n",
    "        loss_list.append(loss)\n",
    "    \n",
    "    all_acc_list.append(max(acc_val_list))\n",
    "    all_loss_list.append(loss_list[-1].cpu().item())\n",
    "    print(\n",
    "        \"Fold {:02d}, Validation Acc. {:.4f}, Loss {:.4f} \".format(\n",
    "        fold_num, acc_val_list[-1], loss_list[-1])\n",
    "        )\n",
    "    fold_num += 1\n",
    "average_accuracy = np.mean(all_acc_list)\n",
    "average_loss = np.mean(all_loss_list)\n",
    "\n",
    "print(\"Accuracy_all {:.4f}, Loss {:.4f}\".format(average_accuracy, average_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
